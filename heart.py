# -*- coding: utf-8 -*-
"""heart.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gscrvm5P1E8tAn-gOKihEXHcTaIk-22x

# Heart Attack Prediction Using Ensemble Learning Models

#### Importing Libraries
"""

# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, auc
import joblib

"""#### Load dataset"""

data = pd.read_csv('/Users/ravindranathchowdary/Chowdary/applied data/Heart_Failure_Details.csv')
data.head()

"""#### Exploratory Data Analysis (EDA)"""

# Check for missing values
print(data.isnull().sum())

# Summary statistics
print(data.describe())

# Plot histograms for numerical variables
data.hist(figsize=(12, 10))
plt.show()

# Plot a correlation heatmap
plt.figure(figsize=(10,8))
sns.heatmap(data.corr(), annot=True, cmap="coolwarm")
plt.title("Correlation Matrix")
plt.show()

"""#### Data Cleaning"""

# Removing outliers from 'creatinine phosphokinase'
Q1 = data['creatinine phosphokinase'].quantile(0.25)
Q3 = data['creatinine phosphokinase'].quantile(0.75)
IQR = Q3 - Q1
data = data[(data['creatinine phosphokinase'] >= (Q1 - 1.5 * IQR)) &
            (data['creatinine phosphokinase'] <= (Q3 + 1.5 * IQR))]

"""#### Split Dataset"""

# Split the dataset into features (X) and target (y)
X = data.drop("death", axis=1)
y = data["death"]

# Split data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""#### Data Normalization"""

# Standardize the data
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

"""#### Model Selection and Training"""

# Logistic Regression
lr_model = LogisticRegression()
lr_model.fit(X_train, y_train)

# Random Forest Classifier
rf_model = RandomForestClassifier()
rf_model.fit(X_train, y_train)

# Support Vector Machine
svm_model = SVC(probability=True)
svm_model.fit(X_train, y_train)

# K-Nearest Neighbors
knn_model = KNeighborsClassifier()
knn_model.fit(X_train, y_train)

# Gradient Boosting Classifier
gb_model = GradientBoostingClassifier()
gb_model.fit(X_train, y_train)

"""#### Evaluate Models"""

# Function to evaluate model
def evaluate_model(model):
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    print(f"Accuracy: {accuracy * 100:.2f}%")
    print("Confusion Matrix:")
    print(confusion_matrix(y_test, y_pred))
    print("Classification Report:")
    print(classification_report(y_test, y_pred))

# Evaluate each model
print("Logistic Regression")
evaluate_model(lr_model)
print("\nRandom Forest")
evaluate_model(rf_model)
print("\nSupport Vector Machine")
evaluate_model(svm_model)
print("\nK-Nearest Neighbors")
evaluate_model(knn_model)
print("\nGradient Boosting")
evaluate_model(gb_model)

"""#### Model Tuning and Optimization (Hyperparameter Tuning with GridSearchCV)"""

from sklearn.model_selection import GridSearchCV

# Parameter grid for Logistic Regression
param_grid_lr = {
    'C': [0.01, 0.1, 1, 10],
    'solver': ['liblinear', 'lbfgs']
}

# Parameter grid for Random Forest
param_grid_rf = {
    'n_estimators': [50, 100, 150],
    'max_depth': [None, 10, 20, 30]
}

# Parameter grid for Support Vector Machine
param_grid_svm = {
    'C': [0.1, 1, 10],
    'kernel': ['linear', 'rbf']
}

# Parameter grid for Gradient Boosting
param_grid_gb = {
    'n_estimators': [50, 100, 150],
    'learning_rate': [0.01, 0.1, 0.5]
}

# Creating dictionary to hold models and their parameter grids
models_param_grid = {
    'LogisticRegression': (LogisticRegression(), param_grid_lr),
    'RandomForestClassifier': (RandomForestClassifier(), param_grid_rf),
    'SVC': (SVC(probability=True), param_grid_svm),
    'GradientBoostingClassifier': (GradientBoostingClassifier(), param_grid_gb)
}

# Dictionary to hold the best models after tuning
tuned_models = {}

# Perform grid search for each model
for model_name, (model, param_grid) in models_param_grid.items():
    grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy', n_jobs=-1)
    grid_search.fit(X_train, y_train)
    tuned_models[model_name] = grid_search.best_estimator_
    print(f"{model_name} best parameters: {grid_search.best_params_}")
    print(f"{model_name} best cross-validation score: {grid_search.best_score_:.4f}")
    print("\n")

"""#### Model Comparison and Selection"""

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve

# Function to evaluate and print metrics for each model
def evaluate_model(model, model_name):
    y_pred = model.predict(X_test)
    y_pred_prob = model.predict_proba(X_test)[:, 1] if hasattr(model, "predict_proba") else None

    # Metrics
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    auc = roc_auc_score(y_test, y_pred_prob) if y_pred_prob is not None else None

    # Print metrics
    print(f"\n{model_name} Performance:")
    print(f"Accuracy: {accuracy:.4f}")
    print(f"Precision: {precision:.4f}")
    print(f"Recall: {recall:.4f}")
    print(f"F1 Score: {f1:.4f}")
    if auc is not None:
        print(f"AUC-ROC: {auc:.4f}")

    return accuracy, precision, recall, f1, auc

# Dictionary to store model performance metrics
performance_metrics = {}

# Evaluate each tuned model
for model_name, model in tuned_models.items():
    metrics = evaluate_model(model, model_name)
    performance_metrics[model_name] = metrics

# Convert performance metrics dictionary to DataFrame
metrics_df = pd.DataFrame(performance_metrics, index=['Accuracy', 'Precision', 'Recall', 'F1 Score', 'AUC-ROC']).T
print("\nModel Performance Comparison:\n", metrics_df)

# Select the best model based on highest AUC-ROC or F1 Score
best_model_name = metrics_df['AUC-ROC'].idxmax()
best_model = tuned_models[best_model_name]
print(f"\nSelected Best Model: {best_model_name}")

"""#### Final Model Evaluation and Visualization"""

from sklearn.metrics import ConfusionMatrixDisplay

# Plot confusion matrix for the best model
ConfusionMatrixDisplay.from_estimator(best_model, X_test, y_test, cmap='Blues')
plt.title(f"Confusion Matrix for {best_model_name}")
plt.show()

# ROC Curve for the best model
if hasattr(best_model, "predict_proba"):
    y_pred_prob = best_model.predict_proba(X_test)[:, 1]
    fpr, tpr, _ = roc_curve(y_test, y_pred_prob)
    roc_auc = roc_auc_score(y_test, y_pred_prob)

    plt.figure()
    plt.plot(fpr, tpr, label=f"{best_model_name} (AUC = {roc_auc:.2f})")
    plt.plot([0, 1], [0, 1], 'k--')
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")
    plt.title("ROC Curve")
    plt.legend(loc="lower right")
    plt.show()

"""#### Save the Best Model for Deployment (for future development)"""

# Save the best model
joblib.dump(best_model, f"{best_model_name}_model.pkl")
print(f"{best_model_name} model saved successfully!")